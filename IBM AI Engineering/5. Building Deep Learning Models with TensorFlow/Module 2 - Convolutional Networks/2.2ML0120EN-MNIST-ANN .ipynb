{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Let's perform classifiction with two Neural Networks on the MNIST dataset.\n",
    "\n",
    " - Multi-Layer Perceptron\n",
    " - Convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is MNIST?\n",
    "\n",
    "It's a database of handwritten digits that has a training set of 60.000 examples and a test set of 10.000 examples. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "### Import with TensorFlow\n",
    "\n",
    "We will get the dataset directly from the TensorFlow API, so it really doesn't contain images. It contain the transformed-into-arrays images.\n",
    "\n",
    "One-Hot encoding will be used for the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot\n",
    "<pre>\n",
    "Number representation:    5\n",
    "One-hot encoding:        [5]   [4]    [3]    [2]    [1]    [0]  \n",
    "Array/vector:             1     0      0      0      0      0   \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fast overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example: The first \"image\" of the dataset:\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.38039219  0.37647063\n",
      "  0.3019608   0.46274513  0.2392157   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.35294119  0.5411765\n",
      "  0.92156869  0.92156869  0.92156869  0.92156869  0.92156869  0.92156869\n",
      "  0.98431379  0.98431379  0.97254908  0.99607849  0.96078438  0.92156869\n",
      "  0.74509805  0.08235294  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.54901963  0.98431379  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.99607849  0.99607849  0.99607849  0.74117649  0.09019608\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.88627458  0.99607849  0.81568635\n",
      "  0.78039223  0.78039223  0.78039223  0.78039223  0.54509807  0.2392157\n",
      "  0.2392157   0.2392157   0.2392157   0.2392157   0.50196081  0.8705883\n",
      "  0.99607849  0.99607849  0.74117649  0.08235294  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.14901961  0.32156864  0.0509804   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.13333334  0.83529419  0.99607849  0.99607849  0.45098042  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.32941177  0.99607849  0.99607849  0.91764712  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.32941177  0.99607849  0.99607849  0.91764712  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.41568631  0.6156863   0.99607849  0.99607849  0.95294124  0.20000002\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.09803922  0.45882356  0.89411771\n",
      "  0.89411771  0.89411771  0.99215692  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.94117653  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.26666668  0.4666667   0.86274517\n",
      "  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.99607849  0.99607849  0.55686277  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.14509805  0.73333335  0.99215692\n",
      "  0.99607849  0.99607849  0.99607849  0.87450987  0.80784321  0.80784321\n",
      "  0.29411766  0.26666668  0.84313732  0.99607849  0.99607849  0.45882356\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.44313729\n",
      "  0.8588236   0.99607849  0.94901967  0.89019614  0.45098042  0.34901962\n",
      "  0.12156864  0.          0.          0.          0.          0.7843138\n",
      "  0.99607849  0.9450981   0.16078432  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.66274512  0.99607849  0.6901961   0.24313727  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.18823531\n",
      "  0.90588242  0.99607849  0.91764712  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.07058824  0.48627454  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.32941177  0.99607849  0.99607849  0.65098041  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.54509807  0.99607849  0.9333334   0.22352943  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.82352948  0.98039222  0.99607849  0.65882355  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.94901967  0.99607849  0.93725497  0.22352943  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.34901962  0.98431379  0.9450981   0.33725491  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01960784  0.80784321  0.96470594  0.6156863   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.01568628  0.45882356  0.27058825  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print('For example: The first \"image\" of the dataset:\\n %s' %mnist.train.images[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the imported data\n",
    "\n",
    "The imported data can be divided as follow:\n",
    "\n",
    " - Training (mnist.train): Use the given dataset with inputs and related outputs for training of NN.\n",
    "      - 55.000 observations\n",
    "      - mnist.train.images for inputs\n",
    "      - mnist.train.labels for outputs\n",
    " - Validation (mnist.validation): The same as training\n",
    "      - 5.000 observations\n",
    "      - mnist.validation.images for inputs\n",
    "      - mnist.validation.labels for outputs\n",
    " - Test (mnist.test): The model does not have access to this informations prior to the test phase. It is used to evaluate the performance and accuracy of the model.\n",
    "     - 10.000 observations\n",
    "     - mnist.test.images for inputs\n",
    "     - mnist.test.labels for outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st: Multi-Layer Perceptron\n",
    "\n",
    "Simple type of Neural Network to perform classification tasks on the MNIST digits dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive session\n",
    "\n",
    "Instead of do all the set-up and then execute a session to evaluate tensors and run operations (as usual) we will use and interactive session to create the code and run on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Placeholders\n",
    "\n",
    "Note: The 'shape' argument defines the tensor size by its dimensions\n",
    "\n",
    "**Placeholder 'X':** Represents the input images\n",
    " - Each image is 28 x 28 px -> 784 pixels\n",
    " - 1st dimension: Indicates the **batch size** => None (any size)\n",
    " - 2nd dimension: Indicates the **number of pixels (features in general)** on a single flattened MNIST image => 784 (28*28)\n",
    " \n",
    "**Placeholder 'Y':** Represents the output labels\n",
    " - 10 possible classes (0,1,2,3,4,5,6,7,8,9)\n",
    " - 1st dimension: Indicates the **batch size** => None (any size)\n",
    " - 2nd dimension: Indicates the number of **possible classes** => 10\n",
    " \n",
    "**dtype:**: In general, use tf.float32. The limitation is that some functions only accepts float32 or float64 representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (55000, 784)\n",
      "Num instances: 55000 \n",
      "Num features: 784\n",
      "Training dataset labels shape: (55000, 10)\n",
      "Num possible classes: 10\n"
     ]
    }
   ],
   "source": [
    "print('Training dataset shape:', mnist.train.images.shape)\n",
    "numTrainInstances = mnist.train.images.shape[0]\n",
    "numFeatures = mnist.train.images.shape[1]\n",
    "print('Num instances: %d \\nNum features: %d' %(numTrainInstances,numFeatures))\n",
    "print('Training dataset labels shape:', mnist.train.labels.shape)\n",
    "numLabels = mnist.train.labels.shape[1]\n",
    "print('Num possible classes: %d' %(numLabels))\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32,shape=[None,numFeatures],name='x_placeholder')\n",
    "y_ = tf.placeholder(dtype=tf.float32,shape=[None,numLabels],name='y_gold_placeholder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Bias\n",
    "\n",
    "This time, with zeros initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Weight Tensor\n",
    "W = tf.Variable(tf.zeros(dtype=tf.float32,shape=[numFeatures,numLabels]))\n",
    "#Bias Tensor\n",
    "b = tf.Variable(tf.zeros(dtype=tf.float32,shape=[numLabels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize variables (Note that we are in an interactive session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Weights and Biases to input\n",
    "\n",
    "Representation of the operations that are: A matrix multiplication between x (inputs) and W (weights) and posterior biases add\n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/88ksiymk1xkb10rgk0jwr3jw814jbfxo.png\" alt=\"HTML5 Icon\" style=\"width:350px;height:306px;\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mathematical representation of image above\n",
    "tf.matmul(x,W) + b;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression\n",
    "\n",
    "Softmax is an **activation function** that is normally used in classification problems. \n",
    "\n",
    "It \"squashes\" a K-dimensional vector of arbitrary real values to a K-dimensional vector of real values in the range [0, 1] that add up to 1. It generates a distribution of probabilities for the output, for example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0 -->.0.1%  \n",
    "1 -->...2%  \n",
    "2 -->...3%  \n",
    "3 -->...2%  \n",
    "4 -->..12%  \n",
    "5 -->..10%  \n",
    "6 -->..57%\n",
    "7 -->..20%\n",
    "8 -->..55%\n",
    "9 -->..80%  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SoftMax\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic function is used for the binary classification 0/1\n",
    "\n",
    "Softmax function is a generalized type of logistic function that can output a multiclass categorical probabilty distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "It is a function that is used to **minimize the difference between the right answers and estimated outputs** by the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**But before let's see what the next functions do**\n",
    "\n",
    "The following code shows an example of cross-entropy for a minibatch of size 2 with 3 possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some good predictions give as a cross-entropy of...: 0.105\n",
      "Some bad predictions give as a cross-entropy of...: 0.805\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_gold_test = [[1.0,0.0,0.0],[1.0,0.0,0.0]]\n",
    "\n",
    "\n",
    "output_test = [[0.9,0.1,0.1],[0.9,0.1,0.1]]\n",
    "print('Some good predictions give as a cross-entropy of...: %.3f' % np.mean(-np.sum(y_gold_test * np.log(output_test),1)))\n",
    "\n",
    "output_test = [[0.5,0.2,0.1],[0.4,0.3,0.1]]\n",
    "print('Some bad predictions give as a cross-entropy of...: %.3f' % np.mean(-np.sum(y_gold_test * np.log(output_test),1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reduce_sum** computes the sum of elements of **(y_ * tf.log(y))** across second dimension of the tensor, and **reduce_mean** computes the mean of all elements in the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cost function\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y),axis=[1])) \n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "#axis=1 => The outputs, columns (axis=0 are the observations, rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of optimization: Gradient Descent\n",
    "\n",
    "Configure the optimizer. There are several but Gradient Descent will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training batches\n",
    "\n",
    "Train using minibatch Gradient Descent.\n",
    "\n",
    "Gradient Descent will find the **global minimum** but it's **computationally expensive** so minibatches will be used.\n",
    "\n",
    "First, let's take a brief look on the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch inputs shape:  (50, 784)\n",
      "The first image on batch:\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          1.\n",
      "  0.50196081  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          1.\n",
      "  0.74901962  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.25098041\n",
      "  1.          0.50196081  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.25098041\n",
      "  1.          1.          0.50196081  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.50196081  1.          1.          0.50196081  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  1.          1.          1.          0.25098041  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.74901962  1.          1.          1.          0.          0.          0.\n",
      "  0.          0.25098041  1.          0.74901962  0.74901962  0.50196081\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          1.          1.          1.          0.25098041  0.          0.\n",
      "  0.          0.74901962  1.          1.          1.          1.          1.\n",
      "  0.74901962  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.25098041  1.          1.          1.          0.          0.          0.\n",
      "  0.50196081  1.          1.          1.          0.74901962  0.74901962\n",
      "  1.          1.          0.50196081  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.25098041  1.          1.          1.          0.50196081  0.          0.\n",
      "  0.25098041  1.          1.          1.          0.50196081  0.\n",
      "  0.50196081  1.          1.          0.50196081  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.50196081  1.          1.          1.          0.          0.\n",
      "  0.25098041  1.          1.          0.74901962  0.25098041  0.          0.\n",
      "  0.50196081  1.          1.          0.25098041  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.74901962  1.          1.          0.50196081  0.          0.\n",
      "  0.74901962  1.          1.          0.25098041  0.          0.\n",
      "  0.25098041  1.          1.          0.50196081  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          1.          1.          1.          0.50196081\n",
      "  0.          1.          1.          1.          1.          0.          0.\n",
      "  0.          0.74901962  1.          0.74901962  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          1.          1.          1.          0.\n",
      "  0.25098041  1.          1.          1.          0.          0.          0.\n",
      "  0.74901962  1.          1.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          1.          1.          1.          0.\n",
      "  0.74901962  1.          1.          0.74901962  0.          0.25098041\n",
      "  0.74901962  1.          1.          0.74901962  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.25098041  1.          1.\n",
      "  0.50196081  0.50196081  1.          1.          1.          0.          0.\n",
      "  1.          1.          1.          0.74901962  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          1.          1.\n",
      "  0.74901962  0.50196081  1.          1.          1.          0.50196081\n",
      "  1.          1.          1.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.74901962  1.          1.          0.74901962  1.          1.          1.\n",
      "  1.          1.          0.74901962  0.25098041  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.25098041  1.          1.          1.          1.          1.          1.\n",
      "  1.          0.74901962  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.25098041  1.\n",
      "  0.74901962  0.50196081  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.        ]\n",
      "Batch labels shape:  (50, 10)\n",
      "The label of the first image on batch:\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "batch = mnist.train.next_batch(50)\n",
    "# print all dictionaries in the list\n",
    "batch_inputs = batch[0]\n",
    "batch_labels = batch[1]\n",
    "\n",
    "print('Batch inputs shape: ',batch_inputs.shape)\n",
    "print('The first image on batch:\\n %s' % batch_inputs[0])\n",
    "print('Batch labels shape: ',batch_labels.shape)\n",
    "print('The label of the first image on batch:\\n %s' %batch_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch summary**\n",
    "\n",
    "It's a tuple where:\n",
    " - batch[0] => Observations\n",
    "  - batch[0][0] => First observation\n",
    " - batch[1] => Labels\n",
    "  - batch[1][0] => First observation label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "Let's get some results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load 50 training examples for each training iteration\n",
    "batch_size = 50\n",
    "epochs = 1000\n",
    "for step in range(epochs):\n",
    "    #Get batch\n",
    "    batch = mnist.train.next_batch(batch_size)\n",
    "    \n",
    "    #Prepare feed dict => We need to feed [x,y_]\n",
    "    feed = {x : batch[0], y_ : batch[1]}\n",
    "    \n",
    "    #Run the optimizer\n",
    "    train_step.run(feed_dict=feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final accuracy for the simple ANN model is: 91.35\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "acc = accuracy.eval(feed_dict={x : mnist.test.images, y_ : mnist.test.labels}) * 100\n",
    "\n",
    "print('The final accuracy for the simple ANN model is: %.002f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#End session - Good bye\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
